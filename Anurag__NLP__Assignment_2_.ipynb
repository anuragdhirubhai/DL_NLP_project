{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "be93ce92e9cb4358aa34c6562c068824": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d4675e3a3a5481ea446c56416caf76f",
              "IPY_MODEL_0781922cda4345ca904ddd802f91b4f2",
              "IPY_MODEL_2483df34eaaa402a87b5998de8834970"
            ],
            "layout": "IPY_MODEL_825c8411ce574692ba8dd156d788e30c"
          }
        },
        "3d4675e3a3a5481ea446c56416caf76f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_075887d05bda4db9bfe181e8d5f764fd",
            "placeholder": "​",
            "style": "IPY_MODEL_f28d731126be4c45adf7d2310f43491b",
            "value": "100%"
          }
        },
        "0781922cda4345ca904ddd802f91b4f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89366256c2c54297bd3e700431b131d0",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8aaca7b0905641dcb39b7531e1c00638",
            "value": 2
          }
        },
        "2483df34eaaa402a87b5998de8834970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba5796f1d8c14c0c98a7df74f1310b2a",
            "placeholder": "​",
            "style": "IPY_MODEL_ea2bc11e7c664590a9ce81bbe22db1dc",
            "value": " 2/2 [00:00&lt;00:00, 79.50it/s]"
          }
        },
        "825c8411ce574692ba8dd156d788e30c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "075887d05bda4db9bfe181e8d5f764fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f28d731126be4c45adf7d2310f43491b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89366256c2c54297bd3e700431b131d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aaca7b0905641dcb39b7531e1c00638": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba5796f1d8c14c0c98a7df74f1310b2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea2bc11e7c664590a9ce81bbe22db1dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anuragdhirubhai/DL_NLP_project/blob/main/Anurag__NLP__Assignment_2_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DL _ NLP Assignment 2 _ Anurag_Pandey_059\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "CBeV1iU_bQkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Zero-shot evaluation?\n",
        "\n",
        "Zero-shot evaluation: In the context of machine learning and especially natural language processing, zero-shot learning or evaluation refers to the model's ability to correctly interpret and respond to tasks or queries that it has never seen during its training phase. In other words, the model does not receive any specific examples or training on the exact task at hand but is still expected to perform the task correctly. It's a true test of a model's generalization ability."
      ],
      "metadata": {
        "id": "qktbk1tTYjpS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is few-shot evaluation?\n",
        "\n",
        "Few-shot evaluation: Few-shot learning or evaluation, on the other hand, means that the model is given a handful (few) examples of the task it's supposed to perform. Based on these few examples, the model needs to generalize and correctly perform similar tasks. This approach is closer to the way humans often learn – we don't always need thousands or millions of examples to understand a concept or perform a task; often, a few good examples are enough."
      ],
      "metadata": {
        "id": "lly4WCdXYomz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is in-context learning?\n",
        "\n",
        "In-context learning: This term refers to the learning approach where models use the immediate context to make predictions or decisions. The context may include a few preceding sentences, the previous dialogues in a conversation, or any other relevant and immediately preceding data. GPT-3, for instance, leverages in-context learning by using the past several tokens (words, roughly speaking) to predict the next token in a sequence."
      ],
      "metadata": {
        "id": "LttPC_LFYt7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the performance you were able to achieve?\n",
        "\n",
        "Performance: Based on the provided average F1 score of 0.383, and Average BLEU score: 0.1015657605339988 it can be deduced that GPT-3's zero-shot performance on a randomly selected set of 50 examples from the SQuAD v2 validation set is modest. The F1 score lies between 0 (worst) and 1 (best), so a score of 0.383 suggests there's significant room for improvement in the model's performance. It should be noted that question answering is a challenging task and this was a zero-shot evaluation (no specific training or fine-tuning on the task), so a modest score isn't unexpected."
      ],
      "metadata": {
        "id": "sCJ16sE8Y8Uy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Is it higher or lower than DistilBERT? Why do you think that is the case?\n",
        "\n",
        "It is lower.\n",
        "Evaluating a model's performance based on a small sample size, such as 50 examples, might not provide a fully representative understanding of its capability. A larger, more diverse set of examples could yield a more accurate assessment of the model's performance across various types of questions and contexts.\n",
        "The F1 score, which was used to measure performance in this case, primarily focuses on the overlap of individual words in the predicted and actual answers. It doesn't take into account the order of words, nor does it understand the semantics or meaning behind them. Therefore, it might not perfectly represent the quality of the answers generated by the model.\n",
        "\n",
        "In question answering tasks, a model could generate an answer that is semantically correct and makes perfect sense in the given context, but uses different words or phrasing compared to the actual answer. In such cases, the F1 score could be misleadingly low."
      ],
      "metadata": {
        "id": "v7wRfUNIZFEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "id": "8FaGuAusWcXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "id": "0XIHz8nWWhLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import random\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = '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'\n",
        "\n"
      ],
      "metadata": {
        "id": "zimp-GhIWstr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the SQUAD V2 dataset\n",
        "squad_v2 = load_dataset('squad_v2')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "be93ce92e9cb4358aa34c6562c068824",
            "3d4675e3a3a5481ea446c56416caf76f",
            "0781922cda4345ca904ddd802f91b4f2",
            "2483df34eaaa402a87b5998de8834970",
            "825c8411ce574692ba8dd156d788e30c",
            "075887d05bda4db9bfe181e8d5f764fd",
            "f28d731126be4c45adf7d2310f43491b",
            "89366256c2c54297bd3e700431b131d0",
            "8aaca7b0905641dcb39b7531e1c00638",
            "ba5796f1d8c14c0c98a7df74f1310b2a",
            "ea2bc11e7c664590a9ce81bbe22db1dc"
          ]
        },
        "id": "AljzqF2zWwcU",
        "outputId": "46ed0fec-5497-43e1-b4af-da1ee3feff86"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be93ce92e9cb4358aa34c6562c068824"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Select 50 random examples from the validation set\n",
        "random_samples = random.choices(squad_v2['validation'], k=10)\n",
        "\n",
        "# Store the model's predictions\n",
        "predictions = []"
      ],
      "metadata": {
        "id": "L-KkXL0fW1DL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through the examples and get GPT-3's answers\n",
        "for sample in random_samples:\n",
        "    # Construct the prompt\n",
        "    prompt = f\"{sample['context']}\\nQuestion: {sample['question']}\\nAnswer:\"\n",
        "    \n",
        "    # Get the model's response\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=prompt,\n",
        "        temperature=0.5, # You can tweak this to adjust the randomness of the output\n",
        "        max_tokens=100   # Adjust this based on how long you expect the answers to be\n",
        "    )\n",
        "\n",
        "    # Store the output (minus the 'Answer:' prefix) as the model's prediction\n",
        "    predictions.append(response['choices'][0]['text'].strip().removeprefix('Answer:').strip())\n",
        "\n",
        "# Now, predictions holds GPT-3's answers to the 50 questions\n",
        "\n"
      ],
      "metadata": {
        "id": "DyZV2TnRWW4e"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"\n",
        "    Lower text and remove punctuation, articles and extra whitespace.\n",
        "    \"\"\"\n",
        "    import string, re\n",
        "\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n"
      ],
      "metadata": {
        "id": "uFaq9TUDXatU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_score_single(predicted, true):\n",
        "    predicted_tokens = normalize_answer(predicted).split()\n",
        "    true_tokens = normalize_answer(true).split()\n",
        "\n",
        "    common = Counter(predicted_tokens) & Counter(true_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "\n",
        "    precision = 1.0 * num_same / len(predicted_tokens)\n",
        "    recall = 1.0 * num_same / len(true_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    return f1\n",
        "\n",
        "f1s = []\n",
        "for prediction, sample in zip(predictions, random_samples):\n",
        "    # There may be multiple correct answers for some questions\n",
        "    if len(sample['answers']['text']) > 0:  # Check if there are answers\n",
        "        sample_f1s = [f1_score_single(prediction, answer) for answer in sample['answers']['text']]\n",
        "        # Take the maximum F1 score as the sample's score\n",
        "        f1s.append(max(sample_f1s))\n",
        "    else:\n",
        "        # If there are no correct answers, the prediction is correct if and only if it's also empty\n",
        "        f1s.append(float(prediction.strip() == ''))\n",
        "\n",
        "# Compute the average F1 score\n",
        "average_f1 = sum(f1s) / len(f1s)\n",
        "print('Average F1 score:', average_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0uBQ4U3XdqV",
        "outputId": "e8f5c344-27fd-430e-d922-9c3710e03415"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average F1 score: 0.014285714285714287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the average F1 score\n",
        "average_f1 = sum(f1s) / len(f1s)\n",
        "print('Average F1 score:', average_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk4ubamZXMUX",
        "outputId": "ef052906-cb94-40cb-9b7a-4b66dfdd0343"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average F1 score: 0.014285714285714287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTaP4z-za2KW",
        "outputId": "12b218c5-3d45-42ef-d0b6-3ab356a84409"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "6gXtupqpanMo"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleus = []\n",
        "\n",
        "for prediction, sample in zip(predictions, random_samples):\n",
        "    # There may be multiple correct answers for some questions\n",
        "    if len(sample['answers']['text']) > 0:  # Check if there are answers\n",
        "        prediction_tokens = word_tokenize(prediction)\n",
        "        reference_tokens = [word_tokenize(answer) for answer in sample['answers']['text']]\n",
        "        # Compute BLEU score\n",
        "        bleu = sentence_bleu(reference_tokens, prediction_tokens)\n",
        "        bleus.append(bleu)\n",
        "    else:\n",
        "        # If there are no correct answers, the prediction is correct if and only if it's also empty\n",
        "        bleus.append(float(prediction.strip() == ''))\n",
        "\n"
      ],
      "metadata": {
        "id": "wgup16x7Xs7v"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the average BLEU score\n",
        "average_bleu = sum(bleus) / len(bleus)\n",
        "print('Average BLEU score:', average_bleu)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsghfcejasoj",
        "outputId": "1759b63a-458a-4664-8721-fb003dafd7df"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU score: 9.109159947227211e-233\n"
          ]
        }
      ]
    }
  ]
}